{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage: How to check the lineage between the RDD  \n",
    "\n",
    "We can track the lineage across wide range of tranformation . We can classify the dependency in two types namely \n",
    "\n",
    "Narrow Dependency : Where each partition of the parent RDD is used by the only one child RDD.\n",
    "ex: Map, Filter ,union , sample ,mapPartitions and Join with input with co-partitions \n",
    "\n",
    "Wide Dependency : Here in wide dependencies have each partition of the parent RDD is used by the multiple child RDD's.\n",
    "ex: groupBYKeys,intersection,distinct,reduceByKey,coalesce,repartition,cartesian and Join with input not co-partitions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations Examples \n",
    "\n",
    "Transformation is an operation applied on RDD to create new RDD ex : maps, filtes, flatmap and etc \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPS\n",
    "\n",
    "We use map transformation when we need to transform a RDD by applying the function on each element.\n",
    "\n",
    "def func(a):\n",
    "  a = a * a \n",
    "  return a\n",
    "rdd.map(func)\n",
    "\n",
    "or we can use lambda function : rdd.map(lambda z : z*z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 25, 36]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext \n",
    "#sc.stop()\n",
    "sc = SparkContext(\"local\",\"name1\")\n",
    "arr = [2,4,5,6]\n",
    "rdd = sc.parallelize(arr)\n",
    "rdd1 = rdd.map(lambda a : a*a)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatmap\n",
    "\n",
    "Flatmap is similar to map but each elements are mapped to 0 to more output elements.\n",
    "Returns new RDD first by applying the function to all the elements of the RDD and then flatten the results.\n",
    " \n",
    "rdd.flatmap(lambda b  : (b,b*b,b+1))\n",
    "see the below example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 8]\n",
      "[4, 16, 100, 5, 25, 100, 6, 36, 100, 8, 64, 100]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc.stop()\n",
    "sc = SparkContext(\"local\",\"name\")\n",
    "\n",
    "rdd = sc.parallelize([4,5,6,8])\n",
    "\n",
    "rdd_flatmap = rdd.flatMap(lambda x : (x,x*x,100))\n",
    "\n",
    "print(rdd.collect())\n",
    "\n",
    "print(rdd_flatmap.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Filters\n",
    "\n",
    "Returns the new RDD containing the only element which satisfy the constraint \n",
    "\n",
    "arr = [2,4,6,8,7,5,9,10,11,13]\n",
    "\n",
    "rdd.filter(lambda x : x%2 == 0)\n",
    "\n",
    "new RDD will create with data which satisfy the x%2 =0 constraint\n",
    "\n",
    "arr1 = ['apple', 'mango','allahabad','andaman','alcohol','bananan','aprk']\n",
    "filter(lambda x : x[0] =='a')\n",
    "\n",
    "returns new RDD contains only elements starts with 'a' letter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'allahabad', 'andaman', 'alcohol', 'aprk']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filter = sc.parallelize([2,4,6,8,7,5,9,10,11,13])\n",
    "rdd_filter1 = rdd_filter.filter(lambda x: x%2 == 0)\n",
    "rdd_filter1.collect()\n",
    "\n",
    "arr1 = ['apple', 'mango','allahabad','andaman','alcohol','bananan','aprk']\n",
    "rdd_flter = sc.parallelize(arr1)\n",
    "rdd_flter1 = rdd_flter.filter(lambda x : x[0] =='a')\n",
    "rdd_flter1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Groupby\n",
    "\n",
    "Group the data in the original RDD and create a pair where key is the output of the user function and value is the all item from which function yeilds the key \n",
    "\n",
    "arr1 = ['apple', 'mango','allahabad','andaman','alcohol','bananan','aprk']\n",
    "\n",
    "creates new RDD with Key and value pairs as below for above input \n",
    "\n",
    "[('a', ['apple', 'allahabad', 'andaman', 'alcohol', 'aprk']), ('b', ['bananan']), ('m', ['mango'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[('a', ['apple', 'allahabad', 'andaman', 'alcohol', 'aprk']), ('m', ['mango']), ('b', ['bananan'])]\n"
     ]
    }
   ],
   "source": [
    "rdd_grpby = rdd_flter.groupBy(lambda x : x[0])\n",
    "print(type(rdd_grpby))\n",
    "val =[(k,list(v))  for (k,v) in rdd_grpby.collect()]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### groupbyKey\n",
    "\n",
    "Group the value for the each key in the RDD Creates the new pair where original key corresponds to the collected group of values \n",
    "\n",
    "arr = [(a,1), (a,3),(a,5) ,(b,3),(b,9),(c,8),(d,4),(d,5)]\n",
    "\n",
    "For the above list if we apply groupbyKey tranformation result will be \n",
    "\n",
    "[('a', [1, 3, 5]), ('b', [3, 9]), ('c', [8]), ('d', [4, 5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 3, 5]), ('b', [3, 9]), ('c', [8]), ('d', [4, 5])]\n"
     ]
    }
   ],
   "source": [
    "grp_arr = [('a',1), ('a',3),('a',5) ,('b',3),('b',9),('c',8),('d',4),('d',5)]\n",
    "grp_rdd = sc.parallelize(grp_arr)\n",
    "grp_rdd1 = grp_rdd.groupByKey()\n",
    "#print(grp_rdd.coolect())\n",
    "\n",
    "print(list((j[0],list(j[1])) for j in grp_rdd1.collect()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAPPARTITIONS\n",
    "\n",
    "mapPartition transformation converts the each partition of the source RDD into many result , In mapPartition map() tranformation is applied on each partition simultaneously .\n",
    "MapPartition is like Map() but difference is it's run separately on each partition of RDD .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print (sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 15, 24]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MapPartition one way selecting 3 blocks of partitions \n",
    "input_data = range(1,10)\n",
    "result_mp = sc.parallelize(input_data, 3)\n",
    "def mapspart(iterator): yield sum(iterator)\n",
    "result_mp.mapPartitions(mapspart).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i have one one default partitions cluster \n",
    "mappar_check = sc.parallelize(input_data)\n",
    "mappar_Cmp = mappar_check.mapPartitions(mapspart)\n",
    "mappar_Cmp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results you may observed  that how mapPartition will work \n",
    "\n",
    "1. In case one where i've taken the 3 blocks into one partitions \n",
    "partition 1 = 1,2,3 => sum = 6\n",
    "partition 2 = 4,5,6 => sum = 15\n",
    "partition 3 = 7,8,9 => sum = 24\n",
    "\n",
    "2. In case two I haven't mentioned the partitons map partition will consider the default partition . I have only one partition \n",
    "partition 1 = 1,2,3,4,5,6,7,8,9 => sum = 45\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
