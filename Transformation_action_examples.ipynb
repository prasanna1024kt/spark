{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=sc.stop()\n",
    "sc = SparkContext(\"local\",\"name\")\n",
    "rdd = sc.textFile(\"files.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP and FlatMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['spark',\n",
       "  'has',\n",
       "  'certain',\n",
       "  'operations',\n",
       "  'which',\n",
       "  'can',\n",
       "  'be',\n",
       "  'performed',\n",
       "  'on',\n",
       "  'rdd.',\n",
       "  'an',\n",
       "  'operation',\n",
       "  'is',\n",
       "  'a',\n",
       "  'method,',\n",
       "  'which',\n",
       "  'can',\n",
       "  'be',\n",
       "  'applied',\n",
       "  'on',\n",
       "  'a',\n",
       "  'rdd',\n",
       "  'to',\n",
       "  'accomplish',\n",
       "  'certain',\n",
       "  'task.',\n",
       "  'rdd',\n",
       "  'supports',\n",
       "  'two',\n",
       "  'types',\n",
       "  'of',\n",
       "  'operations,',\n",
       "  'which',\n",
       "  'are',\n",
       "  'action',\n",
       "  'and',\n",
       "  'transformation.',\n",
       "  'an',\n",
       "  'operation',\n",
       "  'can',\n",
       "  'be',\n",
       "  'something',\n",
       "  'as',\n",
       "  'simple',\n",
       "  'as',\n",
       "  'sorting,',\n",
       "  'filtering',\n",
       "  'and',\n",
       "  'summarizing',\n",
       "  'data.spark',\n",
       "  'is',\n",
       "  'used',\n",
       "  'in',\n",
       "  'fastest',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'hive',\n",
       "  'engine']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_fun(lines):\n",
    "    lines=lines.lower()\n",
    "    lines=lines.split(' ')\n",
    "    return lines\n",
    "map_rdd1 = rdd.map(test_fun)\n",
    "map_rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'has', 'certain', 'operations', 'which']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2= rdd.flatMap(test_fun)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter :\n",
    "\n",
    "Removing the stopword from the file like is ,am ,are , a,the, for . To remove the stop words, we can use a “filter” transformation which will return a new RDD containing only the elements that satisfy given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'has',\n",
       " 'certain',\n",
       " 'operations',\n",
       " 'which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'performed',\n",
       " 'on',\n",
       " 'rdd.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword =['is','am' ,'a', 'are','the','for']\n",
    "filter_rdd = rdd2.filter(lambda x :  x not in stopword)\n",
    "filter_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy : \n",
    "\n",
    "The “groupBy”  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spa', <pyspark.resultiterable.ResultIterable at 0x106cc8a58>),\n",
       " ('has', <pyspark.resultiterable.ResultIterable at 0x106cc8470>)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_rdd = filter_rdd.groupBy(lambda w : w[0:3])\n",
    "groupby_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', <pyspark.resultiterable.ResultIterable at 0x106cc8a20>),\n",
       " ('has', <pyspark.resultiterable.ResultIterable at 0x106cc8cc0>),\n",
       " ('certain', <pyspark.resultiterable.ResultIterable at 0x106cc8780>),\n",
       " ('operations', <pyspark.resultiterable.ResultIterable at 0x106cc8b38>),\n",
       " ('which', <pyspark.resultiterable.ResultIterable at 0x106cc8240>),\n",
       " ('can', <pyspark.resultiterable.ResultIterable at 0x106cc8c18>),\n",
       " ('be', <pyspark.resultiterable.ResultIterable at 0x106cc84a8>),\n",
       " ('performed', <pyspark.resultiterable.ResultIterable at 0x106cc8ba8>),\n",
       " ('on', <pyspark.resultiterable.ResultIterable at 0x106cc8320>),\n",
       " ('rdd.', <pyspark.resultiterable.ResultIterable at 0x106cc88d0>)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_data = filter_rdd.map(lambda y : (y,1))\n",
    "map_data.take(10)\n",
    "groupby_data = map_data.groupByKey()\n",
    "groupby_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark', <pyspark.resultiterable.ResultIterable object at 0x106cd81d0>), ('has', <pyspark.resultiterable.ResultIterable object at 0x106cd8240>), ('certain', <pyspark.resultiterable.ResultIterable object at 0x106cd8358>), ('operations', <pyspark.resultiterable.ResultIterable object at 0x106cd8438>), ('which', <pyspark.resultiterable.ResultIterable object at 0x106cd82e8>), ('can', <pyspark.resultiterable.ResultIterable object at 0x106cd84e0>), ('be', <pyspark.resultiterable.ResultIterable object at 0x106cd8668>), ('performed', <pyspark.resultiterable.ResultIterable object at 0x106cd8048>), ('on', <pyspark.resultiterable.ResultIterable object at 0x106cd8828>), ('rdd.', <pyspark.resultiterable.ResultIterable object at 0x106cd8128>)]\n"
     ]
    }
   ],
   "source": [
    "print (list((j[0],j[1]) for j in groupby_data.take(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which', 'which', 'which']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_rdd.filter(lambda x : x =='which').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the each word in the file : \n",
    "\n",
    "The “mapValues” transformation is like a map (can be applied on any RDD) transform but it has one difference that when we apply map transform on pair RDD we can access the key and value both of this RDD but in case of “mapValues” transformation, it will transform the values by applying some function and key will not be affected. So for example, in below code I applied sum, which will calculate the sum (counts) for the each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1), ('has', 1), ('certain', 2), ('operations', 1), ('which', 3)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fregency_check = groupby_data.mapValues(sum).map(lambda x : (x[0],x[1]))\n",
    "fregency_check.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the each word in the file using reduceByKey:\n",
    "\n",
    "The “reduceByKey” transformations first combined the values for each key in all partition, so each partition will have only one value for a key then after shuffling, in reduce phase executors will apply operation for example, in my case sum(lambda x: x+y).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1), ('has', 1), ('certain', 2), ('operations', 1), ('which', 3)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_key_frquency = map_data.reduceByKey(lambda x,y: x+y).map(lambda x : (x[0],x[1]))\n",
    "reduce_key_frquency.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in case of “groupByKey” transformation, it will not combine the values in each key in all partition it directly shuffle the data then merge the values for each key. Here in “groupByKey” transformation lot of shuffling in the data is required to get the answer, so it is better to use “reduceByKey” in case of large shuffling of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions\n",
    "\n",
    "The “mapPartitions” is like a map transformation but runs separately on different partitions of a RDD. So, for counting the frequencies of words ‘which’ and ‘certain’ in each partition of RDD.\n",
    "\n",
    "I am creating the function to calculate which will count the frequency of the words then passing function to mapPartition transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_frquency_check(iterator):\n",
    "    \n",
    "    count_word1=0\n",
    "    count_word2=0\n",
    "    \n",
    "    for i in iterator:\n",
    "        if i == 'which':\n",
    "            count_word1 +=1\n",
    "        if i == 'certain':\n",
    "            count_word2 +=1\n",
    "    \n",
    "    return(count_word1,count_word2) \n",
    "filter_rdd.mapPartitions(word_frquency_check).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Check the number of partition in the RDD : \n",
    "\n",
    "filter_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce:\n",
    "\n",
    "To reduce the number of partition in the RDD Coalesce is used to reduce the partition and it's will creates new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'has', 'certain', 'operations', 'which']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_part=filter_rdd.coalesce(1)\n",
    "reduce_part.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions: \n",
    "\n",
    "I have already covered the some action in above practice \n",
    "1. Collect \n",
    "2. Take\n",
    "3. getNumPartitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce :\n",
    "\n",
    "A reduce action is use for aggregating all the elements of RDD by applying pairwise user function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_4 = sc.parallelize(range(1,2000))\n",
    "reduce_check = rdd_4.reduce(lambda x,y: x+y)\n",
    "reduce_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
